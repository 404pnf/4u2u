<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<!--
/**
 * @file Import HTML Module for Drupal
 * @author Dan Morrison http://coders.co.nz/
 * @version $Id: import_html_help.html,v 1.1.2.3 2009/05/09 12:45:16 dman Exp $
 */
-->
<link type="text/css" rel="stylesheet" media="all" href="docs.css" />

<style type="text/css">
/*<![CDATA[*/
      .todo {color:00AA00;}
      .beta {color:#007700;}
/*]]>*/
</style>

<title>Drupal Module: Import_HTML</title>
</head>

<body id="content">
<h1>Drupal Module: Import_HTML</h1>

<h2>Synopsis</h2>

<p>Facility to import an existing, static HTML site structure into
Drupal Nodes.</p>

<p>This is done by allowing an admin to define a <a href="#source_siteroot">source directory (siteroot)</a> of a
traditional HTML website, and importing (as much as possible) the
content and structure into a Drupal site.</p>

<p>Files will be absorbed completely, and their existing cross-links
should be maintained, whilst the standard headers, chrome and navigation
blocks should be stripped and replaced with Drupal equivalents. Old
structure will be inferred and imported from the old folder hierarchy.</p>

<ul>
  <li><a href='#Requirements'>Requirements</a></li>

  <li><a href='#Usage'>Usage</a> Detailed step-by-step</li>

  <li><a href='#Intent'>Intent</a> What it's intended to to

  <ul>
    <li><a href='#Methodology'>Methodology</a> Exactly how it does it, at
    a coder level</li>

    <li><a href='#Notes'>Notes</a> Issues arising, and some detailed
    explanations</li>
  </ul>
  </li>

  <li><a href='#Guide'>Guide</a> Reference section

  <ul>
    <li><a href='#Setup'>Setup</a> Requirements, and Installing for the
    first time</li>

    <li><a href='#Templates'>Import Templates</a> XSL. With great power
    comes great complexity</li>

    <li><a href='#Settings'>Settings</a> Explanation of the user settings</li>
  </ul>
  </li>

  <li><a href='#Development'>Development / TODO</a></li>
</ul>

<h2><a name='Requirements' id="Requirements">Requirements</a></h2>

<h3>Before you begin</h3>

<p>See the <a href="#setup">setup section</a> for details. Because of
the number of settings, this is not just a point-and-go module.</p>

<ul>
  <li>XML/XSLT support on the server. Check your php_info(). (XML under
  PHP 4.3 has some limitations, <a href="#trouble">see below</a>.)</li>

  <li>HTMLTidy - Either with the PHP module or the commandline version.</li>

  <li>Some understanding of XSL for advanced template translation.</li>

  <li>Some libraries of my own (bundled) to actually do the XSLT</li>
</ul>

<h2><a name='Usage' id="Usage">Usage</a></h2>

<p>This module uses no database tables of its own. It requires XML
support on the server, this can be tricky if it's not already enabled.</p>

<p>Given a working system, the process is thus:</p>

<ol>
  <li>Visit the <a href='&base_url&admin/build/import_html/settings'>admin/build/import_html/settings</a>
  page and <a href='#Options'>check the settings</a>.</li>

  <li>If all values look OK for now, you can try a test run by visiting
  admin/build/import_html/demo . Choose a 'page' sort of page, not a portal or
  layout-rich sort of thing. The demo will scrape the given file and
  import it to the system. Some of the new navigation features will not
  be apparent yet, as they apply only to large-scale imports, or at least
  imports that have a defined <a href="#source_siteroot">source siteroot</a>.</li>

  <li>Try opening the '<a href='&base_url&admin/build/import_html'>admin/build/import_html</a>'
  main page and defining a source folder. Enter the <a href="#source_siteroot">root path</a> of the site
  you wish to import and continue. <a href="#Treeview">The UI should
  display a treeview</a> of the files you can selectively select for
  import. <br />
  It's recommended to just try one page at a time to begin with. <br />
  <strong>Note:</strong> If your server has <a
    href="http://nz2.php.net/manual/en/features.safe-mode.php#ini.open-basedir">PHP
  open_basedir</a> restrictions in effect, the webserver/PHP process may
  be prevented from accessing files outside of webroot. <a
    href="#open_basedir">See below</a></li>

  <li>Upon importing a page, a new node should be created. The object of
  the import templates is to trim down the <i>content</i> block to its
  unique value. This will <i>probably</i> require some template tuning,
  so make a new template (copy the existing html2simplehtml.xsl), select it
  (enter the new name in the admin page) <a
    href="http://www.dpawson.co.uk/xsl/sect2/sect21.html">tweak the XSL</a>
  and try again. <br />
  If you are extremely lucky, or don't care too much about the extras,
  you can go straight to bulk import.</li>

  <li>If you need to check how the the images are turning up, they can
  safely be imported as well using the previous interface. They will be
  copied, structured in the same folders they were in originally, into
  the directory configured in the admin/setting. Imported pages will have
  their <a href="#link_rewriting">links rewritten</a> to find them there. <br />
  Two type of content are being imported, depending on file suffix.
  'Pages' (html) - which become nodes ... and everything else, which
  becomes 'files'. </li>

  <li>When you are happy that the body field is as tidy as it's going to
  get (test several pages), you can try a bulk import. This may fill up
  your node collection a bit, so be prepared to delete them if things
  don't work perfectly first time. Many static sites have whole sections
  that are not structured the same as the rest of the pages.</li>

  <li>On input, a menu structure and a bunch of aliases will be
  auto-generated. These can be manually adjusted easily. For instance,
  the menu branches will initially be named after the document titles
  found in the directory structure. Which is great if you used a decent
  folder heirachy, but some of the labels can probably be tidied up a
  bit. For that matter, after input, you can safely re-arrange the menu
  structure altogether, shifting whole sections to different places
  without worrying about links breaking. These changes will show through
  in the menu, sitemap and breadcrumbs <em>but not in the pathalias</em>
  which will reman old-style. <span class='beta'>There appear to be
  issues navigating to pages deep in a menu where the parent has not been
  imported or created yet. This is normal Drupal behaviour when making
  menu links to non-existant paths.</span></li>
</ol>

<p>By following these instructions, you should probably be able to end
up with a version of the old content in the new layout. For large sites
(200+ pages) some extra tuning may be neccessary, eg using different
templates for different sources.</p>

<p>Incremental imports, processing just sections at a time, or repeated
imports as you tune the content or the transformation should be
non-destructive. Re-importing the same file will retain the same node ID
path, and any Drupal-specific additions made so far.</p>

<p>Multiple "Import Profiles" can be set up and saved alongside each other.
This allows you to run side-by-side imports of different sources without 
over-writing the settings each time. You may find you require one profile
for importing the 'product information' part of a subsite, and another for 
importing the 'documentation archive' subdirectory.
It is <em>mainly</em>em> provided as a convenience to macro automation 
tools, and the normal user should just work with the 'default' profile and 
ignore the multi-profile feature.
</p>

<h2><a name='Intent' id="Intent">Intent / Theory</a></h2>

<p>This is intended as a run-once sort of tool, that, once tuned right
on a handful of pages, can churn through a large number of reasonably
structured, reasonably formatted pages doing a lot of the boring copy
&amp; paste that would otherwise be required.</p>

<p>The existing file paths of the source content will be used to create
an automatic menu, and therefore a heirachical structure identical to
the source URLs. With path.module, appropriate aliases will also be
created such that this will enable a drupal instance to TRANSPARENTLY
REPLACE an existing static site without breaking any bookmarks!</p>

<h3><a name='Methodology' id='Methodology'>Methodology Overview / Tasks</a></h3>

<p>A peek under the hood into what happens in what order</p>

<ul>
  <li>We have a facility for spidering/enumerating existing source files. (the
  admin/build/import_html page)</li>

  <li>Define import rules - choose an XSL stylesheet, set some parameters
  on it, <span class='todo'>configure presets for the imported pages</span>.</li>

  <li>Expose selective selection of files to import (admin UI) <br />
  <img src='&path&import_html_select_files.png' alt='[screenshot]' /></li>

  <li>Import each source file by way of sequential :

  <ol>
    <li>(Optional) <span class='beta'>download/copy of files to local
    mirror site.</span></li>

    <li>Processing with html-tidy, to prepare for XSL transforms</li>

    <li>URL-rewriting via XSL. All hrefs are redirected to the new
    pseudo-location aliases, all srcs are redirected to somewhere under <kbd>/files</kbd>.</li>

    <li>Content-scraping via XSL (XSL stylesheet will probably have to be
    customized to each source site)</li>

    <li>Or content-scraping via RegExps and heuristics</li>

    <li>Deduction (as much as possible) of meta-information like page
    title, author,date</li>
    
    <li><a href="#import_to_modules">Extra information can be added as hooks</a>
    specific to core or contrib modules.
    </li>

    <li>Validate nodes and save them with node-insert calls</li>

    <li>Extra API-insert calls (eg to create menu navigation and path aliases) 
    are also called via module-specific hooks after save 
    (required once we know what the new node ID is).
    </li>
  </ol>
  </li>

  <li>Pages are now first-class nodes, and can be administered through
  the CMS as usual.</li>
</ul>

<h3><a name='Notes' id="Notes">Notes</a></h3>

<p>The more valid and more homogenous the source site is, the better. A
creation using strict XHTML and useful, semantic tags like <kbd>#title</kbd>
<kbd>#content</kbd> or something could be imported swiftly. One with a
variety of table structures may not... <br />
Of course, this tool is supposed to be useful when dealing with messy,
non-homogenous legacy sites that need a makeover. <span class='todo'>Sometimes
regular expression parsing may come to the rescue for content
extraction, but that's not implimented yet.</span></p>

<p>I'm choosing XSL because I know it, it's powerful for converting
content out of (well-structured) HTML, and I've had success with this
approach in the past. Others may object to this abstract technology (XSL
is NOT an easy learning curve) but the alternative options include
RegExp wierdness or cut and paste. (<span class='todo'>which I may patch
on as alternative methods - or someone else can have a go</span>) Both
approaches I've also used successfully in bulk site templating (over
THOUSANDS of pages) but it's my call. <a href='#Templates'>Making your
own XSL import template</a> is non-trivial.</p>

<p>In the interests of good housekeeping, <b>imported files with spaces in the 
filenames will be renamed to use underscores</b>. 
Although it spaces <em>can</em> be worked around, they just cause trouble in
website URLs. Thus, references to the spaced, or %20 versions of the files
may break. This rewrite can be disabled in the settings.
<br/>Filenames are assumed to be, and will remain, case-sensitive.
</p>

<h2><a name='Guide' id="Guide">Guide</a></h2>

<h3><a name='setup' id="setup">Installation/setup</a></h3>

<h4>XML Support</h4>

<p>The module can use either the PHP4 and PHP5 implimentations (which
are quite different) but the PHP modules <b>do</b> have to be enabled
somehow. This can be tricky as they often require extra libraries to be
put in your path somewhere. Please don't ask me for instructions, every
time I've done it it hurts my head.</p>
<p>If you can see the words XSL or XSLT in your phpinfo() output, You
should be fine. The module will test and warn you anyway.</p>
<p></p>
<p>PHP 4.3 has at least <a href="#trouble">one known bug</a>.</p>
<h4>HTMLTidy Setup</h4>

<p>The module also uses <a href="http://www.w3.org/People/Raggett/tidy/">the
famous HTMLTidy tool</a>. There is now <a
  href="http://www.zend.com/php5/articles/php5-tidy.php">a PHP module
that impliments HTMLTidy natively</a>, but that needs to be installed
and enabled. If you don't have access to that, we can run it from the
command line. Find the <a
  href="http://tidy.sourceforge.net/docs/Overview.html#download">appropriate
binary release of HTMLTidy</a> for your system, and place it in your
PATH, in the modules install directory, or wherever you like, then <em>define
the path to the executable <a href='&base_url&admin/settings/import_html'>in the
settings</a></em>. This works fine under Windows too.</p>
<p>If this sounds complicated, and you have limited access to a Unix
host and need to use it, there is an auto-installer that can attempt to
set up tidy even on a box you don't have login access to.</p>

<p>As mentioned above, the preferred method is to enable the official, 
binary release tidy extension (not the PECL extension if you can help it).
On some distros (Windows, Redhat) this is just a matter of uncommenting
<code>extension=tidy.so</code> in your php.ini. 
</p>
<h4>Ubuntu-PHP-tidy extension</h4>
<p>
In Ubuntu (as of 2007)
the tidy extension has been left out of the default debian PHP package :( <a href="http://packages.ubuntu.com/feisty/web/php5-tidy">although it may be found in certain repositories?</a>.
<a href="http://www.coggeshall.org/oss/tidy/">Official instructions are to
 recompile php5 from source --with-tidy</a> but that's a bit scary if you are used to using a package manager.
<br/>
Instead, <a href="http://ubuntuforums.org/showthread.php?t=195636">this post
gives instructions on how to compile just the extension, then add it to php</a>.
<small>
I also had to <code>apt-get php5-dev</code> to get "phpize" 
on a brand new clean system, and had to use 
<code>./configure --with-tidy=tidy-20051018/</code> 
instead of just <code>./configure</code>
</small>
</p>

<h3><a name='Templates' id="Templates">Import Templates</a></h3>

<p>An import template defines the mapping between existing HTML content
and our node values. It uses the XSL language because of the power it
has to select bits of a structured document, for example <code>select=\"//*[@id='content']\"</code>
... will find the block anywhere in the page, of any type with the id
'content', and <code>select=\"//table[@class='main']//td[position(3)]\"</code>
Will locate the third TD block in the table called 'main'. Both these
examples would be common when trying to extract the actual <em>text</em>
from a legacy site.</p>

<p>You can begin with the example XSL template, this contains code that
attempts to translate a page containing the usual HTML structures like
(either title or h1) and (either the div called 'content' or the entire
body tag) into a standard, minimal, vanilla, sematically-tagged HTML
doc.</p>

<p>It's likely that whatever site you are importing will NOT be shaped
exactly like we need it to translate straight using this format. You
have to identify the parts of your existing pages that can reliably be
scanned for to define content, then come up with an <a
  href='http://www.w3.org/TR/xpath'>XPath expression</a> to represent
this.</p>

<p>If your source, for example, didn't use nice H1 tage to denote the
page title, but instead always looked like</p>
<pre>
&lt;font size='+2'&gt;&lt;B&gt;my
  page&lt;/B&gt;&lt;/font&gt;
</pre>
<p>... your template could be made to find it, wherever it was in the
page using <code>select=\"//font[@size='+2']/B\"</code> and proceed to
use that as the node title.</p>

<p>No, the code is not pretty, and if Regular Expressions are a foreign
language to you, This is worse. <br />
But <em>this</em> is why developers have been ranting for the last ten
years about using semantic markup!! <br />
The uniformity, and the usefulness of the metadata detected in the
source files will play a big part here.</p>

<p>It's easier to develop and test the XSLT using a third-party tool, I
recommend <a href="http://www.xmlcooktop.com/">Cooktop</a> on Windows, 
or <a href="http://www.oxygenxml.com/">oXygen</a> on everything else. 
Be sure to set the XSL engine to 'Sablotron' which is the one that PHP 
uses under the hood.</p>

<p><span class='todo'>Although it would be <em>possible</em> to
configure a logical mapping system to select different import templates
based on different content, at this stage the administrator is expected
to be doing a bit of hand-tweaking, and predicting all possible inputs
is impossible.</span> <span class='beta'>Some of this sort of logic <em>can</em>
however be built into the powerful XSL template, if you are good at XSL</span></p>

<p>Once importing is taking place, you can even filter it more to
improve the structure of the input, for example by removing all
redundant FONT tags, or by ensuring that every H1,2,3 tag has an
associated #ID for anchoring. Yay XSL.</p>


<h4 id='import_to_taxonomy'>Import to Taxonomy</h4>
<p>If you have taxonomy enabled and the source is tagged, these terms can be imported.
Links with a rel='tag' attribute will be taken to refer to keywords, tags or terms in your available taxonomy.
Each of the following syntaxes should be equivalently detected as the term 'Interesting':
</p>
<pre>
  &lt;a href='whatever/term' rel='tag' &gt;interesting&lt;/a&gt;
  &lt;link href='whatever/term' rel='tag' title='interesting' /&gt;
  &lt;meta name="keywords" content="interesting" /&gt; 
</pre>
<p>
First - if an existing term of that name exists in any valid vocabulary, the imported page will be tagged with it.
If not, the <em>first</em> available freetagging vocabulary will be used to insert the tag.
If no freetagging is enabled, only pre-existing terms will be used.
</p>

<h4 id='import_to_taxonomy'>Import to Chosen Taxonomy</h4>
<p>
Although very rarely used, it's possible to specify the target vocab with syntax such as:
<pre>
  &lt;a href='whatever/term' rel='tag' &gt;subject:interesting&lt;/a&gt;
  &lt;a href='places/77' rel='tag' &gt;location:aotearoa&lt;/a&gt;
</pre>
Which will place the page as 'Interesting' in the 'Subject' vocabulary and 'Aotearoa' in the 'Location' vocabulary.
Raw imports probably won't have this level of namespace, but you can use it to translate contextual information in the page into import clues using the XSL template.
EG, this can be translated quite easily in XSL:
<pre>
  &lt;div class='subjects'&gt;
    &lt;h3&gt;More:&lt;h3&gt;
    &lt;a href='whatever/term/13' rel='tag' &gt;Interesting&lt;/a&gt;
    &lt;a href='whatever/term/funny' rel='tag' &gt;Funny&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class='location'&gt;
    &lt;h3&gt;Places:&lt;h3&gt;
    &lt;a href='places/aotearoa' rel='tag' &gt;Aotearoa&lt;/a&gt;
  &lt;/div&gt;
</pre>

<h4 id='import_to_cck'>Import to CCK</h4>
<p>The base functionality supports placing found content into the <code>$node->body</code>
field, not naturally into any arbitrary CCK fields, but this is also
possible.</p>
<p>If you have a CCK node with (eg) fields:</p>
<pre>field_text, field_byline, field_image</pre>
and your input pages are nice and semantically tagged, eg
<pre>
&lt;body&gt;
  &lt;h1 id='title'&gt;the title&lt;/h1&gt;
  &lt;div id='image'&gt;&lt;img src='this.gif'/&gt;&lt;/div&gt;
  &lt;h3 id='byline'&gt;By me&lt;/h3&gt;
  &lt;div id='text'&gt;the content html etc&lt;/div&gt;
&lt;/body&gt;
</pre>
<p>A mapping from HTML ids to CCK fields will be done automatically, and
the content <em>should</em> just fall into place.</p>
<pre>
  $node-&gt;title = "the title";
  $node-&gt;field_image = "&lt;img src='this.gif'/&gt;&lt;";
  $node-&gt;field_byline = "By me";
  $node-&gt;field_text = "the content html etc";
</pre>
<p>
(Actually, current CCK field notation internally places all field values 
into an array with a 'value'. This is correctly supported during import
via the <code>modules/content.inc:content_import_html()</code>code> hook.
.)
</p>

<h4>Import sidebar/pullquote</h4>
<p>It's common that imported source may contain related non-body content you want to capture.</p>
<p>First - edit your target content type to include a multiline, multivalue text field called 'field_sidebar'</p>
<p>Any source data with the class or id 'sidebar' will now arrive into that textarea. The XSL may need to be adjusted, eg like so:</p>
<pre>
  &lt;xsl:template name="sidebar" match="//*[id='leftcol']"&gt;  
    &lt;div id="sidebar"&gt;
      &lt;xsl:apply-templates /&gt;
    &lt;/div&gt;
  &lt;/xsl:template>
</pre>
<p>The above snippet will find any 'leftcol' content and store it into your own 'sidebar' field.
This field can then be rendered as you wish within Drupal, 
eg by using cck_block.module to put it BACK into a column within your theme :-)
<br/>This method can be extended a lot.
</p> 

<p>In fact, ANY element found in the source text with an ID or class
gets added to the <code>$node</code> object during import, although most
data found this way is immediately discarded again if the content type
doesn't know how to serialize it. Enabling debugging will display the full
node object as it exists before saving. Inspecting that data structure may help 
you tune a storage space in your target content type.
<br />
A special-case demonstrated here prepends <code>field_</code> to known
CCK field names. Normally they get labelled as-is.</p>
<p>If the source data is NOT tagged, you'll have to develop a bit of
custom XSL to produce the same effect.</p>
<h4>customtemplate2simplehtml.xsl</h4>
<pre>
... xsl preamble ...
  &lt;xsl:template name="html_doc" match="/"&gt;  
    &lt;html&gt;
    &lt;body&gt;
    ... other extractions ...
    &lt;h3 id="byline"&gt;
      &lt;xsl:value-of select="./descendant::xhtml:img[2]/@alt" /&gt;
    &lt;/h3&gt;
    &lt;/body&gt;
  &lt;/html&gt;
&lt;/xsl:template>
</pre>
<p>In this example, the byline we wanted to extract was the alt value of
the second image found in the page (a real-world example). This has now
been extracted and wrapped in an ID-ed h3 during an early phase of the
import process, and should now turn up in the CCK field_byline as
desired. <br />
XSL is complex, but magic.</p>

<h4 id='import_to_modules'>Import to Other Modules</h4>
<p>
Any add-on modules can use a hook to extract their own data from the 
input file and add data to the node object.
<br/>
The <code>modules/</code> directory contains the callbacks that allow
any module access to the half-cooked node object and raw source data to 
extract and insert/modify its own node properties before it is saved.
<br/>
Contrib examples are in the <code>import_html/modules</code> directory.
See that for the hook prototype and docs.
</p>
<p>
If, for example you were able to extract date/time information out of an 
import page, event.module could be told to do so, 
and create detailed event nodes.
</p>

<h4>Other XML!</h4>
<p>
I've sucessfully used it to import other random XML formats
(RecipeML) although the advantages of doing so are currently limited.
</p>
<p>
If it is possible for you to create an XSL template that translates any
arbitrary XML dialect into the 'simple' HTML + microformat markup used 
during the import phase (see examples) then your XML can be imported into 
Drupal nodes.
<b>It is supported for one source file to produce multiple Nodes</b>.
The 'simple' HTML half-way phase should be an XML document containing
multiple HTML elements. Wrapping them in xt:document nodes is a good idea.
(Example needs to be given)
</p>

<h3><a name='Settings' id="Settings">Settings</a></h3>

<p>On the <a href='&base_url&admin/build/import_html/settings'>
Administer - Site building - Import HTML</a> screen, you can (if you wish):</p>

<ul>
  <li>Choose the <a href='#Templates'>import template</a>. These
  templates translate between the existing page structure and the raw
  content blocks. XSL templates are supplied in the modules '<code>templates/</code>'
  directory. Place your own templates there. <br/>Use the existing examples
  to start from. <code>simplehtml2simplehtml.xsl</code> is the easiest to build on.
  <code>html2simplehtml.xsl</code> attempts to be generic, so includes a bunch 
  of catch-all logic.
  </li>

  <li>Customize a parameter used in the import template - the id of the
  real 'content' block of the source documents. <span class='todo'>This
  could be extended into a wizard to work towards an all-purpose
  template, but that will probably never happen. Can't predict how broken
  the import sites may be.</span></li>

  <li>When a site is imported, it must bring along some of its baggage.
  Images and suchlike. You can choose where they will end up as the 
  <b>Extra File Storage Path</b>.</li>

  <li>When the imported site is given new URLs (reflecting the original
  path) we can publish the new nodes in a 'subsite' by applying a prefix
  directory to the aliases they are issued. The existing old links will
  be written to point to where the imported neighbouring pages are
  EXPECTED to end up. Incremental processing means they may not always be
  there until the whole site is done. Link checking (preferably
  on-the-fly) would be a nice tidy-up process.</li>

  <li>The new URLs generated for the new pages are url_aliases based on
  the original paths. You can choose to have tidy (no suffix) or legacy
  (old .htm or whatever suffix) aliases - or both.</li>

  <li>As the input is (a fragment of) Pure HTML, the content filter
  (input format) must be set correctly. I choose to define a blank
  filter, which doesn't even add extra BRs, but you can override that if
  you wish. </li>

  <li>If using non-native HTMLTidy support, the path to the tidy
  executable should be defined. 
  PHP safe mode or settings can cripple this, so you may be out of 
  luck on cheap hosting servers.
  </li>
</ul>

<h4><a name='Treeview' id="Treeview">Notes on the Treeview Interface</a></h4>

<p>Files and folders beginning with _ or . are nominally 'hidden' so are
skipped and do not show up on this listing. While it's possible to list
a thousand or so files, It may be a good idea to allow the listing to be
more selective, to scale to larger sites. Do this by entering the <strong>Subsection
to list</strong> <em>before</em> clicking list and waiting for every
file on the server to be enumerated.</p>

<h2><a name='Development' id="Development">Development / TODO</a></h2>

<p>As mentioned in Usage, this module uses no database tables of its
own. Pages are read straight into 'page' nodes. </p>


<p>It's easy to imagine this sytem set up as a synchroniser, that could
re-fetch and refresh local nodes when remote content changes. This would
involve recording exactly what the source URL was (which isn't currently
done) but would be a fun feature.</p>

<p>I may fork off the page-parsing into a pluggable method, so that a
regexp version can be developed alongside, and be used for folk without
XSL support.</p>

<p>How to leverage this to import a local site to a remote server? You
must either unpack the source files somewhere on that machine, then
provide the absolute path where the server can find them, or <span
  class='todo'>upload a zip package and I'll try to take it from
there.(TODO)</span></p>

<p class='todo'>Also TODO is a 'Spidering' method to try to import URL
sites. Way in the future!</p>


<p class='todo'>TODO There are issues when a page links directly to a
file that would be regarded as a resource via an href. Most hrefs are
re-written to point to the new node, but things like large images or
word docs get imported under 'files'. The XSL rewrite_href_and_src.xsl
attempts to correct for this, but there may be some side-effects. Always
run a link checker after import.</p>

<h3 id='trouble'>Trouble</h3>

<p>The PHP4 XML parser (Sablotron) has trouble with duplicate attributes
- if found in a tag (like from old bad HTML) all subsequent input will
be flattened to plaintext. Older versions of HTMLTidy, however, do not
detect and fix these for us. Make sure that tidy supports <a
  href="http://tidy.sourceforge.net/docs/quickref.html#repeated-attributes">option
repeated-attributes</a>. It seems the commandline version fixed this
somewhere between the 2000 and 2004 release. (Not sure about the PHP
module version - it's PHP5, so should be OK)</p>
<p><a href="http://drupal.org/node/97532">An issue has been found</a>
running under</p>
<pre>PHP 4.3.11
xsltproc 1.0.16
libxml 2.6.2
libxslt 1.1.0
</pre>
<p>(possibly other similar configurations) whereby &amp;lt; and &amp;gt;
encoded entities in the input are prematurely converted to the &lt; and
&gt; s outputting unencoded results. We are pretty sure this is a
limitation of the old, incomplete XML implimentations of the time. An
upgrade to PHP 4.4 solved it in one case, so good luck.</p>

<p id="open_basedir">If your server has <a
  href="http://nz2.php.net/manual/en/features.safe-mode.php#ini.open-basedir">PHP
open_basedir</a> restrictions in effect, the webserver/PHP process may
be prevented from accessing files outside of webroot. This is a good
security measure, but may stop import_html from reading your source data
(even though browsing the source directories may still appear to work).
The <code>open_basedir</code> setting can be seen in your phpinfo. <br />
An error like: <code>Local file copy failed (/tmp/1fixed/simple.htm to
files/imported/simple.htm)</code> When you are sure the source file <strong>does</strong>
exist and permissions <strong>are</strong> readable may be symptomatic
of good security on your server. A reasonable fix is to place your
source data inside webroot/files (even if just temporarily) to run the
import process, then delete it later. Alternatively, copy your data over
top of web root (as described in walkthrough.htm) to do an in-place
import. Disabling open_basedir is not recommended, and probably requires
root privileges anyway. <a href="http://drupal.org/node/103221">Drupal.org
issue discussion</a></p>

<p id="max_allowed_packet"><b>max_allowed_packet</b> :- It has been found that 
there is a limit to how many batch operations you can queue up at one time.
If you get a white screen, and error in your log saying something like:
<code>Got a packet bigger than &#039;max_allowed_packet&#039; bytes</code>
It means the server is trying to do just too damn much. The <em>list of instructions</em>
is too long to fit into one database entry!
The <a href="http://dev.mysql.com/doc/refman/5.1/en/packet-too-large.html">
max_allowed_packet limit can be increased in your MySQL configuration</a> or 
<em>possibly</em> from code. import_html includes an attempt to fix this problem
automatically - but MAY NOT ALWAYS WORK on some hosts.
<br/>
If your host has such a limit, you will have to take imports slowly, and only
select and process a few hundred pages at a time.
</p>

<p>I've gone to great lengths to rewrite the links from the new node
locations to <em>relative</em> links to the resources that moved over
into /files/ but there are problems. When <code>a/long/path/index.html</code>
links to its image by going <code>../../../files/a/long/path/pic.jpg</code>
it <b>works</b> which is good. But as <code>a/long/path/index.html</code>
is also aliased to <code>a/long/path</code> - that up-and-over path is <em>wrong</em>
now the page is being served from what looks to the browser like a
different place.</p>
<p>I don't favour embedding anything that hard-codes the Drupal
base_url, and we don't want to use HTML BASE. I want to continue to
support portable subsites, so embedding site-rooted links (<code>/files/etc</code>)
is not great either.</p>
<p>Currently, by happy chance, going up one <code>../</code> too far
will get <em>ignored</em> by most browsers, so if you are <em>not</em>
running Drupal in a subdirectory, the requests for both style of page <em>will</em>
just work. Which will mean that 80% of cases should get by OK. The rest
may need an output filter of some sort developed some day</p>

<p>
If you find that duplicate, identical menu items are created 
(both '/mycontent' and '/mycontent/mycontent') 
or that child items are created under inaccessable or non-existant parents, 
check the 'Default Document' in the settings. 
The process will interpret 'index.htm' and 'index.html' differently, 
and only the correct one can be used as the parent item. 
Enter the filename appropriate to the site you are importing from. 
<br/>If you are using both interchangably on one site .. :-{
</p>

<h3><a name="glossary">Glossary / Terminology</a></h3>
<dl>
<dt id="source_siteroot">Source Siteroot</dt>
<dd>The root of the input site. This may be a folder on the local filesystem, 
possibly beginning with "/". 
Imported files will calculate their links relative to there. 
If the source files have all their links <em>truly relative</em> 
(no links starting with '/' or 'http') then links will be rewritten as normal. 
If a link is found that starts with '/' or 'http://site.name/' 
(<b>root-relative</b>) then this will be converted to behave as if it were starting from the 
<em>Source Siteroot</em>. 
<br/>
EG: <code>/var/www/old_site/html/</code>
</dd>

<dt id="source_subsection">Source Subsection</dt>
<dd>A folder <em>within Source Siteroot</em> to process. Links will still be recalculated relative to the Source Siteroot, but only the subsection will be displayed for selection. Use this to manage large numbers of files that may take a while to prepare.
EG: <code>archives/2007</code>
</dd>

<dt id="link_rewriting">Link Rewriting</dt>
<dd>Links will be re-written according to the rules in the settings.
<em>No actual link-checking is done</em>, 
the links are rewritten to the location the other files 
<em>are expected to be</em> 
so if you are processing subsections or selections from the tree, 
they may point to places that don't exist yet. 
<br/>This means that imports are 'atomic' and it is safe to import or re-import 
pages individually without knowing what has happened before or will happen.
<br/>You are advised to run a link-checker afterwards.  
<p>There is provision for <strong>pages</strong> and <strong>resources</strong>
 to be placed in different places in the site. 
In general, pages will be accessed under the normal site root, 
while Drupal conventions place images and documents into a 
/sites/site.name/files folder. 
Most legacy sites have images in /images etc. 
This needs to become <code>/sites/site.name/files/images</code> within Drupal.
The rewriting does this, by detecting the <em>suffix</em> 
of the file being linked to in href= or src= tags. 
Any file type that is <em>not</em> in the list of known HTML file types 
is regarded as a 'resource' and rewritten to appear under the 'files' directory. 
<small>The $import_html_file_classes array is currently hard-coded in the module.</small>
<span class='beta'>File suffixes are not good enough
  for this, should the suffix list be editable, or should I scan the
  files themselves?</span>
</p>
<p class='todo'>TODO Hard http://old.site/ links-to-self 
may need to be removed using a different process.</p>

</dd>

</dl>

<h3><a name='reference'>Reference</a></h3>

<p>Long ago, I started building this with reference to the existing <a
  href='http://drupal.org/node/14858'>import/export module</a> but I
couldn't find too many common features. The transitional format the XSL
templates convert into is a 'microformat' of XHTML (basically XHTML, but
with strictly controlled classes and IDs). This is how I see a
platform-agnostic dump of content should be exported, when this
eventually morphs into import_export_HTML.</p>

</body>
</html>
